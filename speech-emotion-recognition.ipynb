{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Speech Emotion Recognition\n\n- The objective of this project is to develop a robust speech emotion recognition system capable\nof accurately classifying the emotional states conveyed in spoken language.\n\n- By analyzing the acoustic features of speech signals, the system should be able to categorize emotions such as\nhappiness, sadness, anger, fear, and more.\n\n![](https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11042-020-10329-2/MediaObjects/11042_2020_10329_Fig1_HTML.png)","metadata":{}},{"cell_type":"code","source":"#Importing the Libraries\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport librosa\nimport seaborn as sns\nplt.style.use('ggplot')\n\nfrom keras.models import Sequential\nfrom keras import layers, optimizers, callbacks\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\nfrom IPython.display import Audio\nfrom pydub import AudioSegment, effects\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-01T10:52:19.629249Z","iopub.execute_input":"2023-10-01T10:52:19.629545Z","iopub.status.idle":"2023-10-01T10:52:36.537489Z","shell.execute_reply.started":"2023-10-01T10:52:19.629516Z","shell.execute_reply":"2023-10-01T10:52:36.536314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the data\n- We'll merge speech data from four datasets into one dataframe, including file paths, gender labels, and emotion labels.\n- Each dataframe's size will be specified, and an example filename with the bolded emotion label will be shown.\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"#vWe will gather speech data from four datasets and store it in a single dataframe along with the corresponding file paths, gender labels and emotion labels. The size of each respective dataframe will be specified, along with an example filename in which the emotion label is bolded. \nRavdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nCrema = \"/kaggle/input/cremad/AudioWAV/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:47.206361Z","iopub.execute_input":"2023-10-01T10:52:47.207935Z","iopub.status.idle":"2023-10-01T10:52:47.213378Z","shell.execute_reply.started":"2023-10-01T10:52:47.207886Z","shell.execute_reply":"2023-10-01T10:52:47.212112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" #### Let's check each dataset & print the shpae & sample of it.","metadata":{}},{"cell_type":"code","source":"#Lets check the Ravdess\nravdess_dir_lis = os.listdir(Ravdess)\npath_list = []\ngender_list = []\nemotion_list = []\n\nemotion_dic = {'03' : 'happy','01' : 'neutral', '04' : 'sad','05' : 'angry', '06' : 'fear','07' : 'disgust',}\n\nfor directory in ravdess_dir_lis:\n    actor_files = os.listdir(os.path.join(Ravdess, directory))\n    for audio_file in actor_files: \n        part = audio_file.split('.')[0]\n        key = part.split('-')[2]\n        if key in emotion_dic:\n            gender_code = int(part.split('-')[6])\n            path_list.append(f\"{Ravdess}{directory}/{audio_file}\")\n            gender_list.append('female' if gender_code & 1 == 0 else 'male')\n            emotion_list.append(emotion_dic[key])\n            \nravdess_df = pd.concat([\n    pd.DataFrame(path_list, columns=['path']),\n    pd.DataFrame(gender_list, columns=['sex']),\n    pd.DataFrame(emotion_list, columns=['emotion'])\n], axis=1)\n\nprint(ravdess_df.shape)\nprint(ravdess_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:47.216679Z","iopub.execute_input":"2023-10-01T10:52:47.217057Z","iopub.status.idle":"2023-10-01T10:52:48.028405Z","shell.execute_reply.started":"2023-10-01T10:52:47.217011Z","shell.execute_reply":"2023-10-01T10:52:48.027038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Crema-D Dataframe\ncrema_dir_list = os.listdir(Crema)\npath_list = []\ngender_list = []\nemotion_list = []\n\nemotion_dic = {'HAP' : 'happy','NEU' : 'neutral','SAD' : 'sad','ANG' : 'angry', 'FEA' : 'fear','DIS' : 'disgust',}\n\nfemale_id_list = [\n    '1002', '1003', '1004', '1006', '1007', '1008', '1009', '1010', '1012', '1013', '1018', \n    '1020', '1021', '1024', '1025', '1028', '1029', '1030', '1037', '1043', '1046', '1047', \n    '1049', '1052', '1053', '1054', '1055', '1056', '1058', '1060', '1061', '1063', '1072', \n    '1073', '1074', '1075', '1076', '1078', '1079', '1082', '1084', '1089', '1091',\n]\n\nfor audio_file in crema_dir_list:\n    part = audio_file.split('_')\n    key = part[2]\n    if key in emotion_dic and part[3] == 'HI.wav':\n        path_list.append(f\"{Crema}{audio_file}\")\n        gender_list.append('female' if part[0] in female_id_list else 'male')\n        emotion_list.append(emotion_dic[key])\n\ncrema_df = pd.concat([\n    pd.DataFrame(path_list, columns=['path']),\n    pd.DataFrame(gender_list, columns=['sex']),\n    pd.DataFrame(emotion_list, columns=['emotion'])\n], axis=1)\n\nprint(crema_df.shape)\nprint(crema_df.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:48.030645Z","iopub.execute_input":"2023-10-01T10:52:48.031814Z","iopub.status.idle":"2023-10-01T10:52:48.498534Z","shell.execute_reply.started":"2023-10-01T10:52:48.031767Z","shell.execute_reply":"2023-10-01T10:52:48.497294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tess Dataframe\ntess_dir_list = os.listdir(Tess)\npath_list = []\ngender_list = []\nemotion_list = [] \n\nemotion_dic = {'happy':'happy','neutral':'neutral', 'sad':'sad','Sad': 'sad','angry' : 'angry','fear': 'fear','disgust': 'disgust',}\n\nfor directory in tess_dir_list:\n    audio_files = os.listdir(os.path.join(Tess, directory))\n    for audio_file in audio_files:\n        part = audio_file.split('.')[0]\n        key = part.split('_')[2]\n        if key in emotion_dic:\n            path_list.append(f\"{Tess}{directory}/{audio_file}\") \n            gender_list.append('female') # female only dataset\n            emotion_list.append(emotion_dic[key])\n            \ntess_df = pd.concat([\n    pd.DataFrame(path_list, columns=['path']),\n    pd.DataFrame(gender_list, columns=['sex']),\n    pd.DataFrame(emotion_list, columns=['emotion'])\n], axis=1)\n\nprint(tess_df.shape)\nprint(tess_df.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:48.500087Z","iopub.execute_input":"2023-10-01T10:52:48.501092Z","iopub.status.idle":"2023-10-01T10:52:49.501548Z","shell.execute_reply.started":"2023-10-01T10:52:48.501048Z","shell.execute_reply":"2023-10-01T10:52:49.500348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"savee_dir_list = os.listdir(Savee)\npath_list = []\ngender_list = []\nemotion_list = []\n\nemotion_dic = {'h'  : 'happy',    'n'  : 'neutral',    'sa' : 'sad','a'  : 'angry',    'f'  : 'fear',    'd'  : 'disgust'}\n\nfor audio_file in savee_dir_list:\n    part = audio_file.split('_')[1]\n    key = part[:-6]\n    if key in emotion_dic:\n        path_list.append(f\"{Savee}{audio_file}\")\n        gender_list.append('male') # male only dataset\n        emotion_list.append(emotion_dic[key])\n        \nsavee_df = pd.concat([\n    pd.DataFrame(path_list, columns=['path']),\n    pd.DataFrame(gender_list, columns=['sex']),\n    pd.DataFrame(emotion_list, columns=['emotion'])\n], axis=1)\n\nprint(savee_df.shape)\nprint(savee_df.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:49.504333Z","iopub.execute_input":"2023-10-01T10:52:49.505310Z","iopub.status.idle":"2023-10-01T10:52:49.748996Z","shell.execute_reply.started":"2023-10-01T10:52:49.505267Z","shell.execute_reply":"2023-10-01T10:52:49.747898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Concatinating all the datasets in df","metadata":{}},{"cell_type":"code","source":"df = pd.concat([ravdess_df,crema_df, tess_df, savee_df], axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:49.750513Z","iopub.execute_input":"2023-10-01T10:52:49.751201Z","iopub.status.idle":"2023-10-01T10:52:49.759384Z","shell.execute_reply.started":"2023-10-01T10:52:49.751136Z","shell.execute_reply":"2023-10-01T10:52:49.758324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets check the sample of new dataset\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:49.761073Z","iopub.execute_input":"2023-10-01T10:52:49.761936Z","iopub.status.idle":"2023-10-01T10:52:49.779175Z","shell.execute_reply.started":"2023-10-01T10:52:49.761897Z","shell.execute_reply":"2023-10-01T10:52:49.777776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exloratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#Distribution of Gender & emotion\nsns.set(style=\"white\")\nsns.countplot(data=df, x='emotion', hue='sex')\nplt.title('Emotion and Gender Distribution')\nplt.xlabel('Emotion')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:49.781049Z","iopub.execute_input":"2023-10-01T10:52:49.782226Z","iopub.status.idle":"2023-10-01T10:52:50.109247Z","shell.execute_reply.started":"2023-10-01T10:52:49.782185Z","shell.execute_reply":"2023-10-01T10:52:50.108075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's go with a female specific model here\ndf = df[df['sex'] == 'female']\nsns.set(style=\"white\")\nax = sns.countplot(data=df, x='emotion',  palette='Set3')\nax.bar_label(ax.containers[0]) \nplt.title('Female Emotion Ditribution')\nplt.xlabel('Emotion')\nplt.ylabel('Count')\nsns.despine()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:50.110959Z","iopub.execute_input":"2023-10-01T10:52:50.111675Z","iopub.status.idle":"2023-10-01T10:52:50.379553Z","shell.execute_reply.started":"2023-10-01T10:52:50.111634Z","shell.execute_reply":"2023-10-01T10:52:50.378457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's drop the sex column as well\ndf.drop('sex', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:50.381181Z","iopub.execute_input":"2023-10-01T10:52:50.381854Z","iopub.status.idle":"2023-10-01T10:52:50.388353Z","shell.execute_reply.started":"2023-10-01T10:52:50.381815Z","shell.execute_reply":"2023-10-01T10:52:50.387226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting the Audio waes","metadata":{}},{"cell_type":"code","source":"import random\nemotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust']\nemotion_colors = ['blue', 'green', 'grey', 'red', 'orange', 'purple']\n\nplt.figure(figsize=(12, 8))\n\ndisplayed_emotions = []\nfor _ in range(len(emotion_labels)):\n    while True:\n        emotion_idx = random.randint(0, len(emotion_labels) - 1)\n        selected_emotion = emotion_labels[emotion_idx]\n        if selected_emotion not in displayed_emotions:\n            break\n    displayed_emotions.append(selected_emotion)\n    emotion_df = df[df.emotion == selected_emotion]\n\n    if not emotion_df.empty:\n        random_sample = emotion_df.sample(1)\n        path = random_sample.iloc[0]['path']\n        y, sr = librosa.load(path)\n        plt.subplot(2, 3, len(displayed_emotions))\n        plt.title(f\"Waveplot for {selected_emotion} emotion\")\n        librosa.display.waveshow(y, sr=sr, alpha=0.7, color=emotion_colors[emotion_idx])\n        display(Audio(path))\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:52:50.392738Z","iopub.execute_input":"2023-10-01T10:52:50.393415Z","iopub.status.idle":"2023-10-01T10:53:06.941203Z","shell.execute_reply.started":"2023-10-01T10:52:50.393375Z","shell.execute_reply":"2023-10-01T10:53:06.940051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note : Audio files are in series as per the graphs","metadata":{}},{"cell_type":"markdown","source":"### \n","metadata":{}},{"cell_type":"code","source":"#Data Preprocessing\ndef preprocess_audio(path):\n    _, sr = librosa.load(path)\n    raw_audio = AudioSegment.from_file(path)\n    \n    samples = np.array(raw_audio.get_array_of_samples(), dtype='float32')\n    trimmed, _ = librosa.effects.trim(samples, top_db=25)\n    padded = np.pad(trimmed, (0, 180000-len(trimmed)), 'constant')\n    return padded, sr","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:53:06.943095Z","iopub.execute_input":"2023-10-01T10:53:06.944210Z","iopub.status.idle":"2023-10-01T10:53:06.951978Z","shell.execute_reply.started":"2023-10-01T10:53:06.944159Z","shell.execute_reply":"2023-10-01T10:53:06.950537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Assigning Labels to each emotion\nemotion_dic = {\n    'neutral' : 0,\n    'happy'   : 1,\n    'sad'     : 2, \n    'angry'   : 3, \n    'fear'    : 4, \n    'disgust' : 5\n}\n\ndef encode(label):\n    return emotion_dic.get(label)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:53:06.953718Z","iopub.execute_input":"2023-10-01T10:53:06.954453Z","iopub.status.idle":"2023-10-01T10:53:06.977519Z","shell.execute_reply.started":"2023-10-01T10:53:06.954413Z","shell.execute_reply":"2023-10-01T10:53:06.976450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since basic EDA & Preprocessing done, lets extract some features\n- Extracting below features: \n- Mel-Frequency Cepstral Coefficients: captures the shape of the spectral envelope of a signal\n- Zero Crossing Rate: captures the number of times a signal changes sign per second\n- Root Mean Square Energy: captures the root mean square amplitude of the audio signal","metadata":{}},{"cell_type":"code","source":"zcr_list = []\nrms_list = []\nmfccs_list = []\nemotion_list = []\nFRAME_LENGTH = 2048\nHOP_LENGTH = 512\n\nfor row in df.itertuples(index=False):\n    try: \n        y, sr = preprocess_audio(row.path)\n\n        zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n        rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=HOP_LENGTH)\n\n        zcr_list.append(zcr)\n        rms_list.append(rms)\n        mfccs_list.append(mfccs)\n\n        emotion_list.append(encode(row.emotion))\n    except:\n        print(f\"Failed for path: {row.path}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-01T10:53:06.979406Z","iopub.execute_input":"2023-10-01T10:53:06.980172Z","iopub.status.idle":"2023-10-01T10:57:55.264687Z","shell.execute_reply.started":"2023-10-01T10:53:06.980116Z","shell.execute_reply":"2023-10-01T10:57:55.262504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.concatenate((\n    np.swapaxes(zcr_list, 1, 2), \n    np.swapaxes(rms_list, 1, 2), \n    np.swapaxes(mfccs_list, 1, 2)), \n    axis=2\n)\nX = X.astype('float32')\n\ny = np.asarray(emotion_list)\ny = np.expand_dims(y, axis=1).astype('int8')\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:57:55.273067Z","iopub.execute_input":"2023-10-01T10:57:55.277394Z","iopub.status.idle":"2023-10-01T10:57:55.430352Z","shell.execute_reply.started":"2023-10-01T10:57:55.277323Z","shell.execute_reply":"2023-10-01T10:57:55.429226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building a LSTM\n\n![](https://www.researchgate.net/publication/324600237/figure/fig3/AS:616974623178753@1524109621725/Long-Short-term-Memory-Neural-Network.png)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Let's setup the data","metadata":{}},{"cell_type":"code","source":"X_train, X_to_split, y_train, y_to_split = train_test_split(X, y, test_size=0.12, random_state=1)\nX_val, X_test, y_val, y_test = train_test_split(X_to_split, y_to_split, test_size=0.3, random_state=1)\n\ny_train_class = to_categorical(y_train, 6)\ny_val_class = to_categorical(y_val, 6)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:57:55.431821Z","iopub.execute_input":"2023-10-01T10:57:55.432893Z","iopub.status.idle":"2023-10-01T10:57:55.467460Z","shell.execute_reply.started":"2023-10-01T10:57:55.432846Z","shell.execute_reply":"2023-10-01T10:57:55.466349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = Sequential()\nMODEL.add(layers.LSTM(64, return_sequences=True, input_shape=(X.shape[1:3])))\nMODEL.add(layers.LSTM(64))\nMODEL.add(layers.Dense(6, activation='softmax'))\n\nprint(MODEL.summary())","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:57:55.468819Z","iopub.execute_input":"2023-10-01T10:57:55.469489Z","iopub.status.idle":"2023-10-01T10:58:01.197887Z","shell.execute_reply.started":"2023-10-01T10:57:55.469428Z","shell.execute_reply":"2023-10-01T10:58:01.197099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rlrop = callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.01, patience=100)\nMODEL.compile(loss='categorical_crossentropy', optimizer='RMSProp', metrics=['categorical_accuracy'])\nmodel = MODEL.fit(X_train, y_train_class, epochs=150, batch_size=16, validation_data=(X_val, y_val_class))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T11:10:51.192249Z","iopub.execute_input":"2023-10-01T11:10:51.192831Z","iopub.status.idle":"2023-10-01T11:20:15.937404Z","shell.execute_reply.started":"2023-10-01T11:10:51.192785Z","shell.execute_reply":"2023-10-01T11:20:15.936214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\naxes[0].plot(model.history['loss'])\naxes[0].plot(model.history['val_loss'])\naxes[0].set_title('Loss for Train and Validation Sets')\naxes[0].set_ylabel('Loss')\naxes[0].set_xlabel('Epochs')\naxes[0].legend(['Training', 'Validation'])\n\naxes[1].plot(model.history['categorical_accuracy'])\naxes[1].plot(model.history['val_categorical_accuracy'])\naxes[1].set_title('Accuracy for Train and Validation Sets')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_xlabel('Epochs')\naxes[1].legend(['Training', 'Validation'])\n\nfig.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T11:20:15.940988Z","iopub.execute_input":"2023-10-01T11:20:15.941815Z","iopub.status.idle":"2023-10-01T11:20:16.492135Z","shell.execute_reply.started":"2023-10-01T11:20:15.941769Z","shell.execute_reply":"2023-10-01T11:20:16.491154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.argmax(MODEL.predict(X_val), axis=1)\nlabels = ['neutral', 'calm', 'sad', 'happy', 'fear', 'disgust']\ncm = confusion_matrix(np.argmax(y_val_class, axis=1), y_pred, labels=range(6))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T11:22:01.203820Z","iopub.execute_input":"2023-10-01T11:22:01.204199Z","iopub.status.idle":"2023-10-01T11:22:01.951341Z","shell.execute_reply.started":"2023-10-01T11:22:01.204139Z","shell.execute_reply":"2023-10-01T11:22:01.950328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model achieved aprox 87% accuracy in classifying the six different emotions.","metadata":{}}]}